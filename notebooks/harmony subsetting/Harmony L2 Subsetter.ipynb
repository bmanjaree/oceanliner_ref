{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harmony EOSS L2SS API Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "Before you beginning this tutorial, make sure you have an account in the Earthdata Login UAT or Production environment, which \n",
    "will be used for this notebook by visiting [https://uat.urs.earthdata.nasa.gov](https://uat.urs.earthdata.nasa.gov).\n",
    "These accounts, as all Earthdata Login accounts, are free to create and only take a moment to set up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Authentication\n",
    "\n",
    "We need some boilerplate up front to log in to Earthdata Login.  The function below will allow Python\n",
    "scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine uat.urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "    \n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "import json\n",
    "import requests\n",
    "import sys\n",
    "import shutil\n",
    "import xarray as xa\n",
    "\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "\n",
    "    Valid endpoints include:\n",
    "        uat.urs.earthdata.nasa.gov - Earthdata Login UAT (Harmony's current default)\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print('Please provide your Earthdata Login credentials to allow data access')\n",
    "        print('Your credentials will only be passed to %s and will not be exposed in Jupyter' % (endpoint))\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass()\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "\n",
    "# GET TOKEN FROM CMR \n",
    "def get_token( url: str,client_id: str, user_ip: str,endpoint: str) -> str:\n",
    "    try:\n",
    "        token: str = ''\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "        xml: str = \"\"\"<?xml version='1.0' encoding='utf-8'?>\n",
    "        <token><username>{}</username><password>{}</password><client_id>{}</client_id>\n",
    "        <user_ip_address>{}</user_ip_address></token>\"\"\".format(username, password, client_id, user_ip)\n",
    "        headers: Dict = {'Content-Type': 'application/xml','Accept': 'application/json'}\n",
    "        resp = requests.post(url, headers=headers, data=xml)\n",
    "        \n",
    "        response_content: Dict = json.loads(resp.content)\n",
    "        token = response_content['token']['id']\n",
    "    except:\n",
    "        print(\"Error getting the token - check user name and password\", sys.exc_info()[0])\n",
    "    return token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find a granule for subsetting\n",
    "\n",
    "Below we call out a specific granule (G1226018995-POCUMULUS) on which we will use the podaac L2 subsetter. Finding this information would complicate the tutorial- but po.daac has a tutorial available for using the CMR API to find collections and granules of interest. Please see the following tutorial for that information:\n",
    "\n",
    "PODAAC_CMR.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "collection = 'C1940473819-POCLOUD'\n",
    "variable = 'sea_surface_temperature'\n",
    "lat_var = 'lat'\n",
    "lon_var = 'lon'\n",
    "venue = 'prod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "cmr_root = 'cmr.earthdata.nasa.gov'\n",
    "harmony_root = 'https://harmony.earthdata.nasa.gov'\n",
    "edl_root = 'urs.earthdata.nasa.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if venue == 'uat':\n",
    "    cmr_root = 'cmr.uat.earthdata.nasa.gov'\n",
    "    harmony_root = 'https://harmony.uat.earthdata.nasa.gov'\n",
    "    edl_root = 'uat.urs.earthdata.nasa.gov'\n",
    "\n",
    "print (\"Environments: \")\n",
    "print (\"\\t\" + cmr_root)\n",
    "print (\"\\t\" + harmony_root)\n",
    "print (\"\\t\" + edl_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the above function to set up Earthdata Login for subsequent requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_earthdata_login_auth(edl_root)\n",
    "token_url=\"https://\"+cmr_root+\"/legacy-services/rest/tokens\"\n",
    "token=get_token(token_url,'jupyter', '127.0.0.1',edl_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Subset of a PO.DAAC Granule\n",
    "\n",
    "We can now build onto the root URL in order to actually perform a transformation.  The first transformation is a subset of a selected granule.  _At this time, this requires discovering the granule id from CMR_.  That information can then be appended to the root URL and used to call Harmony with the help of the `request` library.\n",
    "\n",
    "Above we show how to find a granule id for processing.\n",
    "\n",
    "**Notes:**\n",
    "  The L2 subsetter current streams the data back to the user, and does not stage data in S3 for redirects. This is functionality we will be adding over time.\n",
    "  It doesn't work with URS backed files, which is coming in the next few weeks\n",
    "  it only works on the show dataset, but \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmr_url = \"https://\"+cmr_root+\"/search/granules.umm_json?collection_concept_id=\"+collection+\"&sort_key=-start_date&bounding_box=-90,-45.75,90,-45&token=\"+token\n",
    "\n",
    "response = requests.get(cmr_url)\n",
    "\n",
    "gid=response.json()['items'][0]['meta']['concept-id']\n",
    "print(response.json()['items'][0])\n",
    "print(gid)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxSubsetConfig = {\n",
    "    'collection_id': collection,\n",
    "    'ogc-api-coverages_version': '1.0.0',\n",
    "    'variable': 'all',\n",
    "    'granuleid': gid,\n",
    "    'lat': '(-45.75:45)',\n",
    "    'lon': '(-90:90)'\n",
    "}\n",
    "bbox_url = harmony_root+'/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?granuleid={granuleid}&subset=lat{lat}&subset=lon{lon}'.format(**bboxSubsetConfig)\n",
    "print('Request URL', bbox_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(bbox_url) as response, open('ogc_temp.nc', 'wb') as out_file:\n",
    "    print('Content Size:', response.headers['Content-length'])\n",
    "    shutil.copyfileobj(response, out_file)\n",
    "    print(\"Downloaded request to ogc_temp.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xa.open_dataset('ogc_temp.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "fig, axes = plt.subplots(ncols=3, nrows=math.ceil((len(ds.data_vars)/3)))\n",
    "fig.set_size_inches((15,15))\n",
    "\n",
    "for count, xvar in enumerate(ds.data_vars):\n",
    "    if  ds[xvar].dtype == \"timedelta64[ns]\":\n",
    "        continue\n",
    "        #ds[xvar].astype('timedelta64[D]').plot(ax=axes[int(count/3)][count%3])\n",
    "    ds[xvar].plot(ax=axes[int(count/3)][count%3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the subsetting worked\n",
    "\n",
    "Bounds used were: \n",
    "\n",
    "  'lat': '(-45.75:45)',\n",
    "  'lon': '(-90:90)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "var_ds = ds[variable]\n",
    "msk = np.logical_not(np.isnan(var_ds.data.squeeze()))\n",
    "\n",
    "llat = ds[lat_var].where(msk)\n",
    "llon = ds[lon_var].where(msk)\n",
    "\n",
    "lat_max = llat.max()\n",
    "lat_min = llat.min()\n",
    "\n",
    "lon_min = llon.min()\n",
    "lon_max = llon.max()\n",
    "\n",
    "lon_min = (lon_min + 180) % 360 - 180\n",
    "lon_max = (lon_max + 180) % 360 - 180\n",
    "\n",
    "print(lon_min)\n",
    "print(lon_max)\n",
    "print(lat_min)\n",
    "print(lat_max)\n",
    "\n",
    "if lat_max <= 45 and lat_min >= -45.75:\n",
    "    print(\"Successful Latitude subsetting\")\n",
    "else:\n",
    "    assert False\n",
    "\n",
    "\n",
    "if lon_max <= 90 and lon_min >= -90:\n",
    "    print(\"Successful Longitude subsetting\")\n",
    "else:\n",
    "    assert False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
