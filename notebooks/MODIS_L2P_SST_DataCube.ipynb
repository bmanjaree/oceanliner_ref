{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-you-start\" data-toc-modified-id=\"Before-you-start-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before you start</a></span></li><li><span><a href=\"#Authentication-setup\" data-toc-modified-id=\"Authentication-setup-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Authentication setup</a></span></li><li><span><a href=\"#Hands-off-workflow\" data-toc-modified-id=\"Hands-off-workflow-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Hands-off workflow</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis Ready Data (ARD) workflow for MODIS Aqua L2P SST collection\n",
    "\n",
    "This notebook demonstrates how to created a gridded \"Data Cube\", essentialy an ARD, from native Level 2P sea surface temperature (SST) data from the MODIS Aqua (https://doi.org/10.5067/GHMDA-2PJ19) collection or dataset.  It can also be applied to Terra L2P SST and other similar L2 satellite collections.\n",
    "\n",
    "\n",
    "## Before you start\n",
    "\n",
    "Before you beginning this tutorial, make sure you have an Earthdata account: [https://urs.earthdata.nasa.gov](https://urs.earthdata.nasa.gov) for the operations envionrment (most common) or [https://uat.urs.earthdata.nasa.gov](https://uat.urs.earthdata.nasa.gov) for the UAT environment.\n",
    "\n",
    "Accounts are free to create and take just a moment to set up.\n",
    "\n",
    "## Authentication setup\n",
    "\n",
    "We need some boilerplate up front to log in to Earthdata Login.  The function below will allow Python\n",
    "scripts to log into any Earthdata Login application programmatically.  To avoid being prompted for\n",
    "credentials every time you run and also allow clients such as curl to log in, you can add the following\n",
    "to a `.netrc` (`_netrc` on Windows) file in your home directory:\n",
    "\n",
    "```\n",
    "machine uat.urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating\n",
    "\"netrc access too permissive.\"\n",
    "\n",
    "`$ chmod 0600 ~/.netrc` \n",
    "\n",
    "*You'll need to authenticate using the netrc method when running from command line with [`papermill`](https://papermill.readthedocs.io/en/latest/). You can log in manually by executing the cell below when running in the notebook client in your browser.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:07.928944Z",
     "start_time": "2020-08-11T17:34:07.913616Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "\n",
    "    Valid endpoints include:\n",
    "        uat.urs.earthdata.nasa.gov - Earthdata Login UAT (Harmony's current default)\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print('Please provide your Earthdata Login credentials to allow data access')\n",
    "        print('Your credentials will only be passed to %s and will not be exposed in Jupyter' % (endpoint))\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass()\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:08.212835Z",
     "start_time": "2020-08-11T17:34:08.203168Z"
    }
   },
   "outputs": [],
   "source": [
    "setup_earthdata_login_auth('uat.urs.earthdata.nasa.gov')\n",
    "#setup_earthdata_login_auth('uat.earthdata.nasa.gov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARD workflow\n",
    "\n",
    "The idea of this workflow is to:\n",
    "1) Discover MODIS granules within a region of interest (ROI)\n",
    "\n",
    "2) Make spatial subsets of the dicovered granules\n",
    "\n",
    "3) Remap those subsets to a common grid\n",
    "\n",
    "4) Aggregate the maps to a \"Data Cube\": a co-registed set of images aggregated into a single file the cover the ROI\n",
    "\n",
    "This is done using a combination NASA earthdata search and harmony services, and the python modules for the GDAL image transformation software, and the NCO toolkit. \n",
    "\n",
    "The first code block sets the input datasets, and time and space bounds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:08.712987Z",
     "start_time": "2020-08-11T17:34:08.710091Z"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# GHRSST MODIS Aqua L2P SST v2019.0 collection concept-id (UAT)\n",
    "ccid = \"C1234724470-POCLOUD\"\n",
    "\n",
    "# GHRSST MODIS Aqua L2P SST v2019.0 collection concept-id (CMR)\n",
    "#ccid = \"C1940473819-POCLOUD\"\n",
    "\n",
    "# GHRSST MODIS Terra L2P SST v2019.0 collection concept-id\n",
    "#ccid = \"C1234724471-POCLOUD\"\n",
    "\n",
    "# set the time  bounds  for search\n",
    "start_time = '2020-07-01T00:01:15Z'\n",
    "stop_time = '2020-07-30T00:01:15Z'\n",
    "temporal_coverage = start_time + ',' + stop_time\n",
    "print(temporal_coverage)\n",
    "\n",
    "# set the space bounds\n",
    "# mid Atlantic region\n",
    "#west = -35. ; south = -5. ; east = -25. ; north = 5.\n",
    "\n",
    "# Hawaiian Is.\n",
    "west = -163 ; south = 15 ; east = -153 ; north = 25\n",
    "\n",
    "# Southern California Bight\n",
    "#west = -118 ; south = 32.5 ; east = -117 ; north = 33.5\n",
    "\n",
    "spatial_coverage =  str(west) + ',' + str(south) + ',' + str(east) + ',' + str(north)\n",
    "print(spatial_coverage)\n",
    "\n",
    "# the local download directory\n",
    "download_dir = \"/Users/earmstro/Downloads/modis\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kewyord parameters for the spatial and temporal granule CMR Harmony search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:15.176292Z",
     "start_time": "2020-08-11T17:34:15.171348Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'scroll': \"true\",\n",
    "    'page_size': 2000,\n",
    "    'sort_key': \"-start_date\",\n",
    "    'collection_concept_id': ccid, \n",
    "    'temporal': temporal_coverage,\n",
    "    'bounding_box': spatial_coverage\n",
    "}\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode query parameters and create the complete search url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:16.816986Z",
     "start_time": "2020-08-11T17:34:16.813631Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import makedirs\n",
    "from os.path import isdir, basename\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "from datetime import datetime, timedelta\n",
    "from json import dumps, loads\n",
    "\n",
    "cmr = \"cmr.uat.earthdata.nasa.gov\"\n",
    "#cmr = \"cmr.earthdata.nasa.gov\"\n",
    "\n",
    "query = urlencode(params)\n",
    "url = f\"https://{cmr}/search/granules.umm_json?{query}\"\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of granules found in search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:18.168810Z",
     "start_time": "2020-08-11T17:34:17.709156Z"
    }
   },
   "outputs": [],
   "source": [
    "with urlopen(url) as f:\n",
    "    results = loads(f.read().decode())\n",
    "\n",
    "print(f\"{results['hits']}  granules found in search for collection (dataset): {ccid} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neatly print the first granule record (if one was returned).  \"concept-id\" is the file (granule) reference we will use as input to Harmony subsetting service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:19.667755Z",
     "start_time": "2020-08-11T17:34:19.665059Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(results['items'])>0:\n",
    "    print(dumps(results['items'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the concept-id for each file.  This is the key input for the harmony subsetting API and operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:20.408069Z",
     "start_time": "2020-08-11T17:34:20.404068Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Dump the granule concept Ids\n",
    "file_ids = [r['meta']['concept-id'] for r in results['items']]\n",
    "file_ids\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump the corresponding filenames and location URLs for each concept id for inspection and well being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dump the URLs (containing orignal filenames) for the corresponding files ids\n",
    "\n",
    "file_urls = [[u['URL'] for u in r['umm']['RelatedUrls'] if u['Type']==\"GET DATA\"][0] for r in results['items']]\n",
    "file_urls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-25T12:03:56.780074Z",
     "start_time": "2020-07-25T12:03:56.777273Z"
    }
   },
   "source": [
    "**Perform the subsetting operation using the Earthdata Harmony service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:21.088917Z",
     "start_time": "2020-08-11T17:34:21.085442Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Harmony root\n",
    "harmony_root = 'https://harmony.uat.earthdata.nasa.gov'\n",
    "\n",
    "# Loop over the file (granule) ids\n",
    "for i in range(len(file_urls)):\n",
    "   try:  \n",
    "        harmonyConfig = {\n",
    "            'collection_id': ccid,     \n",
    "            'ogc-api-coverages_version': '1.0.0',\n",
    "            'variable': 'all',\n",
    "            'granuleId': file_ids[i],\n",
    "            'lat': '(' + str(south) + \":\" + str(north) + ')',\n",
    "            'lon': '(' + str(west) + \":\" + str(east) + ')',        \n",
    "            #'subsettingCrs': l2proj_encode,\n",
    "            #'outputCrs': l2proj_encode,\n",
    "            #'interpolation': 'near',\n",
    "            #'width': 1000,\n",
    "            #'height': 1000\n",
    "        }\n",
    "         \n",
    "        # reproject granule    \n",
    "        #harmony_url = harmony_root + '/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?granuleId={granuleId}&interpolation={interpolation}&outputCrs={outputCrs}&height={height}&width={width}'.format(**harmonyConfig)\n",
    "        \n",
    "        # subset granule\n",
    "        harmony_url = harmony_root + '/{collection_id}/ogc-api-coverages/{ogc-api-coverages_version}/collections/{variable}/coverage/rangeset?granuleId={granuleId}&subset=lat{lat}&subset=lon{lon}'.format(**harmonyConfig)\n",
    "        print('Harmony URL', harmony_url)\n",
    "        \n",
    "        # store output in a defined directory with a meaninful filename based on orginal name\n",
    "        out_file = download_dir + 'subset_' + basename(file_urls[i])\n",
    "       \n",
    "        \n",
    "        # -- execute the request --\n",
    "        with request.urlopen(harmony_url) as response, open(out_file, 'wb') as output:\n",
    "           shutil.copyfileobj(response, output) \n",
    "        \n",
    "        \n",
    "   except urllib.error.HTTPError as e:\n",
    "        print(f\"    [{datetime.now()}] FAILURE: {f}\\n\\n{e}\\n{e.read()}\\n\")\n",
    "        raise e                     \n",
    "   except Exception as e:\n",
    "        print(f\"    [{datetime.now()}] FAILURE: {f}\\n\\n{e}\\n\")\n",
    "        raise e\n",
    "   else:\n",
    "        print(f\"    [{datetime.now()}]  SUCCESS for: {out_file}\")\n",
    "        print('    Content Size (bytes):', response.headers['Content-length'])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform resampling/reprojection on subsets using the GDAL module and gdal.Warp().** GDAL will only work on one variable (\"layer\") at a time and also strip out important CF metadata and coordinate variables. Therefore we will use **NCO tricks** to correct these artifacts here and in the next steps. Use the 'pynco' module for NCO python bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, gdalconst\n",
    "from nco import Nco\n",
    "\n",
    "nco = Nco()\n",
    "\n",
    "# keyword args for gdal.Warp()\n",
    "kwargs = {'format': 'netCDF', 'copyMetadata': True, \n",
    "           'outputBounds': [west, south, east, north], \n",
    "           'xRes': 0.01,\n",
    "           'yRes' : 0.01,\n",
    "           'dstSRS':'+proj=longlat +datum=WGS84 +no_defs',\n",
    "         }\n",
    "print(kwargs)\n",
    "\n",
    "nc_vars = ['sea_surface_temperature', 'quality_level']\n",
    "\n",
    "# Loop through subsetted files (use file_urls as the loop list) and warp into defined region from kwargs{}\n",
    "\n",
    "for i in range(len(file_urls)):\n",
    "    for j in range(len(nc_vars)): \n",
    "      variable = nc_vars[j]\n",
    "     \n",
    "      # input filename\n",
    "      src_filename = download_dir + 'subset_' + basename(file_urls[i]) \n",
    "      print(\"source filename: \", src_filename)\n",
    "        \n",
    "      # load the netCDF 'layer' like sea_surface_temperature (variable)\n",
    "      nc_file = 'NETCDF:' + src_filename + ':' + variable\n",
    "      print(nc_file)\n",
    "    \n",
    "      # try/catch for GDAL steps. Dont work on empty netCDF file from subsetting operation  \n",
    "      try:\n",
    "        src = gdal.Open(nc_file, gdalconst.GA_ReadOnly)\n",
    "\n",
    "        #subDatasets = src.GetSubDatasets()\n",
    "        #print('subsdatatsets', subDatasets)\n",
    "\n",
    "        # set output filename\n",
    "        out_filename = download_dir + 'subset_reproject-' + variable +  '-' + basename(file_urls[i]) \n",
    "        ds = gdal.Warp(out_filename, src, **kwargs)\n",
    "        print(\"\")\n",
    "\n",
    "        del ds\n",
    "        del src     \n",
    "        \n",
    "        # add time dimenson to 'Band1' using NCO\n",
    "        nco.ncecat(input=out_filename, output='tmp.nc', options=['-v Band1 -u time'])\n",
    "        nco.ncks(input='tmp.nc', output=out_filename, options=['-v Band1'])\n",
    "\n",
    "        # use NCO to copy (append with -A ) the time variable to the output and make it a record dimension\n",
    "        nco.ncks(input=src_filename, output=out_filename, options=['-v time -A --mk_rec_dmn time'])\n",
    "\n",
    "      except Exception as e:\n",
    "        print(f\"    [{datetime.now()}] FAILURE: \\n\")\n",
    "        #raise e   \n",
    "      else:\n",
    "        print(f\"    [{datetime.now()}]  SUCCESS for: {out_filename}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add variable level CF metadata information that GDAL stripped out using NCO commands.** Use the 'pynco' module for NCO python bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "#import shutil\n",
    "from nco import Nco\n",
    "\n",
    "nco = Nco()\n",
    "\n",
    "sstConfig = {\n",
    "            'scale_factor': 0.005,     \n",
    "            'add_offset': 273.15,\n",
    "            'valid_max': 10000,\n",
    "            'valid_min': -1000,\n",
    "            'long_name': 'sea surface temperature',\n",
    "            'standard_name': 'sea_surface_skin_temperature',\n",
    "            'coverage_content_type': 'physicalMeasurement'\n",
    "            }\n",
    "\n",
    "qualityConfig = {\n",
    "            'valid_max': 0,\n",
    "            'valid_min': 5, \n",
    "            'flag_values': '0b, 1b, 2b, 3b, 4b, 5b',\n",
    "            'flag_meanings': 'no_data bad_data worst_quality low_quality acceptable_quality best_quality',\n",
    "            'long_name': 'quality level of SST pixel',\n",
    "            'coverage_content_type': 'qualityInformation'\n",
    "            }\n",
    " \n",
    "# Atted() method appears broken    \n",
    "#sstOpts = [\n",
    "#             c.Atted(mode=\"o\", attName=\"scale_factor\", varName=variable, Value=0.005, sType='f'), \n",
    "#             c.Atted(mode=\"o\", attName=\"add_offsdt\", varName=variable, Value=273.15, sType='f'), \n",
    "#             c.Atted(mode=\"o\", attName=\"valid_min\", varName=variable, Value=-1000, sType='s'), \n",
    "#]\n",
    "            \n",
    "nc_vars = ['sea_surface_temperature', 'quality_level']\n",
    "os.chdir(download_dir)\n",
    "\n",
    "for i in range(len(nc_vars)): \n",
    "    variable = nc_vars[i]\n",
    "    reg_ex = \"subset*\" + variable + \"*\"\n",
    "    for file in glob.glob(reg_ex):\n",
    "        if variable == 'sea_surface_temperature':\n",
    "          print(\" -> Updating \"+ file)\n",
    "          nco.ncrename(input=file, output=file, options=['-v .Band1,sea_surface_temperature'])\n",
    "        \n",
    "          #sstOpts = {'mode':'o', 'attName':'scale_factor', 'varName':variable, 'Value':0.005, 'sType':'f'}\n",
    "          #nco.ncatted(input=file, output=file, options=sstOpts)\n",
    "\n",
    "          # update the SST variable attributes\n",
    "          nco.ncatted(input=file, output=file, options=['-a scale_factor,'+ variable + ',o,f,{scale_factor}'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a add_offset,'+ variable + ',o,f,{add_offset}'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a valid_min,'+ variable + ',o,s,{valid_min}'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a valid_max,'+ variable + ',o,s,{valid_max}'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a long_name,'+ variable + ',o,c,\"{long_name}\"'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a standard_name,'+ variable + ',o,c,{standard_name}'.format(**sstConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a coverage_content_type,'+ variable + ',o,c,{coverage_content_type}'.format(**sstConfig)])\n",
    "            \n",
    "        elif variable == 'quality_level':\n",
    "          print(\" -> Updating \"+ file)\n",
    "        \n",
    "          nco.ncrename(input=file, output=file, options=['-v .Band1,' + variable])\n",
    "\n",
    "          # update the quality variable attributes\n",
    "          nco.ncatted(input=file, output=file, options=['-a valid_min,'+ variable + ',o,s,{valid_min}'.format(**qualityConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a valid_max,'+ variable + ',o,s,{valid_max}'.format(**qualityConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a long_name,'+ variable + ',o,c,\"{long_name}\"'.format(**qualityConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a flag_values,'+ variable + ',o,c,\"{flag_values}\"'.format(**qualityConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a flag_meanings,'+ variable + ',o,c,\"{flag_meanings}\"'.format(**qualityConfig)])\n",
    "          nco.ncatted(input=file, output=file, options=['-a coverage_content_type,'+ variable + ',o,c,{coverage_content_type}'.format(**qualityConfig)])\n",
    "     \n",
    "    \n",
    "print( \"-Done-\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the MODIS SST Data Cube.** Copy variable(s) to SST target files and catenate all of them into a final output  netCDF file using NCO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "from nco import Nco\n",
    "\n",
    "nco = Nco()\n",
    "os.chdir(download_dir)\n",
    "\n",
    "\n",
    "# Loop through the SST files and add variable quality_level to SST file (target)\n",
    "reg_ex = \"subset_reproject-sea_surface_temperature*\"\n",
    "\n",
    "print( \"Copying quality_level variables to target sst files . . .\")\n",
    "for sst_file in glob.glob(reg_ex):    \n",
    "    quality_file = sst_file.replace(\"sea_surface_temperature\", \"quality_level\")  \n",
    "    nco.ncks(input=quality_file, output=sst_file, options=['-v quality_level -A'])\n",
    "    \n",
    "    \n",
    "# Create the data cube using NCO ncrcat command\n",
    "print(\". . . -Done- \\n\\nCreating MODIS SST data cube . . . \")\n",
    "nco.ncrcat(input=glob.glob(reg_ex), output='MODIS_SST.data-cube.nc')\n",
    "print(\". . . -Done- \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data with xarray and perform some plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T17:34:22.362167Z",
     "start_time": "2020-08-11T17:34:22.359386Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dataCube = download_dir + 'MODIS_SST.data-cube.nc'\n",
    "xds = xr.open_dataset(dataCube)\n",
    "#print(xds)\n",
    "\n",
    "# create objects for subplots\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "\n",
    "# plot a time series at a specific location\n",
    "xds.sea_surface_temperature.isel(lat = 200, lon = 300).plot(ax=axes[0], marker = '.', linestyle = 'None')\n",
    "\n",
    "# a histogram of all points in region of interest\n",
    "xds.sea_surface_temperature.plot(ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.draw()\n",
    "\n",
    "\n",
    "# Filter the dataset using quality information (quality_level value 4 and 5 are best data)\n",
    "qc_dataset = xds.where((xds['sea_surface_temperature'] < 310) & (xds['quality_level'] >= 4))\n",
    "\n",
    "# re plot the time series at a specific location and the regional histogram\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "qc_dataset.sea_surface_temperature.isel(lat = 200, lon = 300).plot(ax=axes[0], marker = '.', linestyle = 'None')\n",
    "qc_dataset.sea_surface_temperature.plot(ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "241.528px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
