{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Case: Study Amazon Estuaries with Data from the EOSDIS Cloud\n",
    "\n",
    "<img src=\"https://cdn.earthdata.nasa.gov/conduit/upload/12946/EOSDISCloud-logo.jpg\" width=\"45%\" />\n",
    "<p><left>Read more about the EOSDIS Cloud at <a href=\"https://earthdata.nasa.gov/eosdis/cloud-evolution\" >NASA Earthdata</a>.</center></p>\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial uses satellite data products to analyze the relationships between river height and land water equivalent thickness in the Amazon River estuary. Users can expand on these examples to also include sea surface salinity, sea surface temperature, and ocean color for example, for a more comprehensive exploration of the Amazon river basinâ€™s estuary and coastal region. \n",
    "\n",
    "The contents are useful for the ocean, coastal and terrestrial hydrosphere disciplines, showcasing how to use on premises and Earthdata cloud datasets, existing Earthdata cloud services and functionalities, and Earthdata User Interface (UI) and Application Programming Interfaces (API). \n",
    "\n",
    "### Learning objectives:\n",
    "\n",
    "* Search for land water equivalent (LWE) thickness (GRACE/GRACE-FO) and river discharge data (MEaSUREs Pre-SWOT)\n",
    "* Access LWE thickeness dataset in Zarr format from Earthdata Cloud (AWS) using the Harmony API (specifically the Zarr reformatted service) \n",
    "* Access discharge dataset from PODAAC on premise (server) data archive \n",
    "* Subset both, plot and compare coincident data.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "The tutorial uses a combination of cloud and on premises datasets:\n",
    "\n",
    "- [**JPL GRACE and GRACE-FO Mascon Ocean, Ice, and Hydrology Equivalent Water Height Coastal Resolution Improvement (CRI) Filtered Release 06 Version 02**](https://podaac.jpl.nasa.gov/dataset/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2)\n",
    "    - Provides land water equivalent (LWE) thickness for observing seasonal changes in water storage around the river. When discharge is high, the change in water storage will increase, pointing to a wet season. Source data are from [GRACE](https://podaac.jpl.nasa.gov/GRACE) and [GRACE-FO](https://podaac.jpl.nasa.gov/GRACE-FO).\n",
    "\n",
    "- [**Pre SWOT Hydrology GRRATS Daily River Heights and Storage Version 2**](https://podaac.jpl.nasa.gov/dataset/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2)\n",
    "    - Provides virtual gauges to stand in for discharge data from Surface Water and Ocean Topography (SWOT). MEaSUREs contains river height products, not discharge, but river height is directly related to discharge and thus will act as a good substitute. Data were produced for the [Pre-SWOT Making Earth System Data Records for Use in Research Environments (MEaSUREs)](https://podaac.jpl.nasa.gov/MEaSUREs-Pre-SWOT) Program.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "This notebook was developed to run in the AWS cloud (us-west-2 region), next to the Earthdata Cloud (PO.DAAC) data holdings, to leverage cloud optimized data formats (e.g. Zarr) and the Earthdata Harmony API, specifically the Zarr refomrating service. For more informaion on Harmony, please see https://harmony.earthdata.nasa.gov/ .\n",
    "\n",
    "In order to currently access PO.DAAC Cloud Pathfinder datasets such as GRACE/GRACE-FO from the Earthdata Cloud, your Earthdata login *username* needs to be added to an restrcited early access list (during the transition period of migrating PO.DAAC data to the Earthdata Cloud). Please contact podaac@podaac.jpl.nasa.gov to make that request.\n",
    "\n",
    "This notebook was developed in Python 3.6 and depends on the following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:47:48.664245Z",
     "start_time": "2020-11-20T21:47:48.065573Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy\n",
    "import zarr\n",
    "import s3fs\n",
    "from IPython.display import HTML\n",
    "from json import dumps, loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoints\n",
    "\n",
    "Should the notebook run against the UAT (user testing) environment? (In most cases users will want to set this flag to *False* to work with publically available, operational datasets.) If you need UAT (testing environment) access (should you have the right permissions), set the flag below to *True* here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UAT = False\n",
    "\n",
    "# The flag above sets endpoints for URS, CMR, & Harmony APIs:\n",
    "if UAT:\n",
    "    cmr = \"cmr.uat.earthdata.nasa.gov\"\n",
    "    urs = \"uat.urs.earthdata.nasa.gov\"\n",
    "    harmony = \"harmony.uat.earthdata.nasa.gov\"\n",
    "else:\n",
    "    cmr = \"cmr.earthdata.nasa.gov\"\n",
    "    urs = \"urs.earthdata.nasa.gov\"\n",
    "    harmony = \"harmony.earthdata.nasa.gov\"\n",
    "\n",
    "cmr, urs, harmony"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA Earthdata Login Setup\n",
    "\n",
    "An Earthdata Login account is required to access data, as well as discover restricted data, from the NASA Earthdata system. Please visit https://urs.earthdata.nasa.gov to register and manage your Earthdata Login account. This account is free to create and only takes a moment to set up.\n",
    "\n",
    "The setup_earthdata_login_auth function will allow Python scripts to log into any Earthdata Login application programmatically. To avoid being prompted for credentials every time you run and also allow clients such as curl to log in, you can add the following to a .netrc (\\_netrc on Windows) file in your home directory:\n",
    "\n",
    "```shell\n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "```\n",
    "   \n",
    "Make sure that this file is only readable by the current user or you will receive an error stating \"netrc access too permissive.\"\n",
    "\n",
    "```shell\n",
    "$ chmod 0600 ~/.netrc\n",
    "```\n",
    "\n",
    "*You will be prompted for your username and password if you dont have a netrc file. Note: these imports are all in the Python 3.6+ standard library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:41:57.314395Z",
     "start_time": "2020-11-20T21:41:52.177000Z"
    }
   },
   "outputs": [],
   "source": [
    "from platform import system\n",
    "from netrc import netrc\n",
    "from getpass import getpass\n",
    "from urllib import request\n",
    "from http.cookiejar import CookieJar\n",
    "from os.path import join, expanduser\n",
    "\n",
    "TOKEN_DATA = (\"<token>\"\n",
    "              \"<username>%s</username>\"\n",
    "              \"<password>%s</password>\"\n",
    "              \"<client_id>PODAAC CMR Client</client_id>\"\n",
    "              \"<user_ip_address>%s</user_ip_address>\"\n",
    "              \"</token>\")\n",
    "\n",
    "\n",
    "def setup_earthdata_login_auth(urs: str='urs.earthdata.nasa.gov', cmr: str='cmr.earthdata.nasa.gov'):\n",
    "\n",
    "    # GET URS LOGIN INFO FROM NETRC OR USER PROMPTS:\n",
    "    netrc_name = \"_netrc\" if system()==\"Windows\" else \".netrc\"\n",
    "    try:\n",
    "        username, _, password = netrc(file=join(expanduser('~'), netrc_name)).authenticators(urs)\n",
    "        print(\"# Your URS credentials were securely retrieved from your .netrc file.\")\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('# Please provide your Earthdata Login credentials for access.')\n",
    "        print('# Your info will only be passed to %s and will not be exposed in Jupyter.' % (urs))\n",
    "        username = input('Username: ')\n",
    "        password = getpass('Password: ')\n",
    "\n",
    "    # SET UP URS AUTHENTICATION FOR HTTP DOWNLOADS:\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, urs, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)\n",
    "\n",
    "    # GET TOKEN TO ACCESS RESTRICTED CMR METADATA:\n",
    "    ip = requests.get(\"https://ipinfo.io/ip\").text.strip()\n",
    "    r = requests.post(\n",
    "        url=\"https://%s/legacy-services/rest/tokens\" % cmr,\n",
    "        data=TOKEN_DATA % (str(username), str(password), ip),\n",
    "        headers={'Content-Type': 'application/xml', 'Accept': 'application/json'}\n",
    "    )\n",
    "    return r.json()['token']['id']\n",
    "\n",
    "\n",
    "# Provide URS credentials for HTTP download auth & CMR token retrieval:\n",
    "_token = setup_earthdata_login_auth(urs=urs, cmr=cmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud data from JPL GRACE and GRACE-FO Mascon\n",
    "\n",
    "![grace mascon](https://podaac.jpl.nasa.gov/Podaac/thumbnails/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2.jpg)\n",
    "\n",
    "**JPL GRACE and GRACE-FO Mascon Ocean, Ice, and Hydrology Equivalent Water Height Coastal Resolution Improvement (CRI) Filtered Release 06 Version 02**\n",
    "\n",
    "This dataset contains gridded monthly global water storage/height anomalies relative to a time-mean, derived from GRACE and GRACE-FO and processed at JPL using the Mascon approach (Version2/RL06). These data are provided in a single data file in netCDF format, and can be used for analysis for ocean, ice, and hydrology phenomena. This version of the data employs a Coastal Resolution Improvement (CRI) filter that reduces signal leakage errors across coastlines. The water storage/height anomalies are given in equivalent water thickness units (cm). The solution provided here is derived from solving for monthly gravity field variations in terms of geolocated spherical cap mass concentration functions, rather than global spherical harmonic coefficients. Additionally, realistic geophysical information is introduced during the solution inversion to intrinsically remove correlated error. Thus, these Mascon grids do not need to be destriped or smoothed, like traditional spherical harmonic gravity solutions. The complete Mascon solution consists of 4,551 relatively independent estimates of surface mass change that have been derived using an equal-area 3-degree grid of individual mascons. A subset of these individual mascons span coastlines, and contain mixed land and ocean mass change signals. \n",
    "\n",
    "For more information, please visit https://grace.jpl.nasa.gov/data/get-data/jpl_global_mascons/. \n",
    "\n",
    "For a detailed description on the Mascon solution, including the mathematical derivation, implementation of geophysical constraints, and solution validation, please see Watkins et al., 2015, doi: 10.1002/2014JB011547. For a detailed description of the CRI filter implementation, please see Wiese et al., 2016, doi: 10.1002/2016WR019344.\n",
    "\n",
    "### Metadata\n",
    "\n",
    "Data from the [TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2](https://podaac.jpl.nasa.gov/dataset/TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2) dataset can be obtained from AWS S3. Use its *ShortName* to retrieve the *collection* metadata from CMR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_ShortName = \"TELLUS_GRAC-GRFO_MASCON_CRI_GRID_RL06_V2\"\n",
    "grace_ShortName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection (dataset)\n",
    "\n",
    "Get the UMM Collection metadata using `requests.get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url=f\"https://{cmr}/search/collections.umm_json\", \n",
    "                 params={'provider': \"POCLOUD\", \n",
    "                         'ShortName': grace_ShortName, \n",
    "                         'token': _token})\n",
    "\n",
    "grace_coll = r.json()\n",
    "grace_coll['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be only one result. Select and print its CMR Search metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_coll_meta = grace_coll['items'][0]['meta']\n",
    "grace_coll_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Granule (file)\n",
    "\n",
    "Get the UMM Granule metadata using `requests.get`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:42:06.135994Z",
     "start_time": "2020-11-20T21:42:05.608696Z"
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get(url=f\"https://{cmr}/search/granules.umm_json\", \n",
    "                 params={'provider': \"POCLOUD\", \n",
    "                         'ShortName': grace_ShortName, \n",
    "                         'token': _token})\n",
    "\n",
    "grace_gran = r.json()\n",
    "grace_gran['hits']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, one result was returned (one *hit*). Print the CMR Search metadata for the granule (`meta`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_gran['items'][0]['meta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other component in each result (from the list of `items`) is the UMM metadata, accessible from the `umm` key. Print the *RelatedUrls* metadata field for the granule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grace_gran['items'][0]['umm']['RelatedUrls']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the URL corresponding to `'Type': 'GET DATA'`. Select the URL from appropriate item in the list, then print:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:42:06.146238Z",
     "start_time": "2020-11-20T21:42:06.143188Z"
    }
   },
   "outputs": [],
   "source": [
    "grace_url = grace_gran['items'][0]['umm']['RelatedUrls'][1]['URL']\n",
    "grace_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to do a regular HTTPS download:\n",
    "    \n",
    "```python\n",
    "r = requests.get(grace_url)\n",
    "with open('tutorial7_data_GRACEFO.nc', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "!ncdump -h tutorial7_data_GRACEFO.nc\n",
    "```\n",
    "\n",
    "But we'll use the Harmony API's Zarr Reformatter service instead of downloading the entire granule. The zarr format will allow us to open and download/read just the data that we require for our Amazon Basin study area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Request to Harmony API: Zarr Reformatter \n",
    "\n",
    "If you have a `jobID` you'd like to re-visit instead of running this command again, modify the cell below to set the *async_jobId* then skip the one immediately after. You can continue from 'Query for the job status and links'.\n",
    "\n",
    "*If you are running for the first time, proceed to the next cells to submit the harmony request.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#async_jobId = \"dfefc536-768c-4db3-a7d2-c326e9253042\"  # jobId belongs to dev. You wont have access.\n",
    "async_jobId = None "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*See important usage note below if this is your first time submitting a request to the Zarr Reformatter service.*\n",
    "\n",
    "The Zarr Reformatter service operates on an input Collection *concept-id* (a CMR construct). The service will accept more user-friendly inputs (like a Collection *ShortName*) in future releases. Here's how you identify the CMR *concept-id* for the JPL GRACE/GRACE-FO Mascon dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_concept_id = grace_coll_meta['concept-id']\n",
    "collection_concept_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of this next cell will only evaluate if there's NOT a valid job identifier set to the `async_jobId` variable above. It submits the Harmony request, and prints the JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async_url = f'https://{harmony}/{collection_concept_id}/ogc-api-coverages/1.0.0/collections/all/coverage/rangeset?format=application/x-zarr'\n",
    "if async_jobId is None:\n",
    "    print('Request URL: ', async_url)\n",
    "    async_response = request.urlopen(async_url)\n",
    "    async_results = async_response.read()\n",
    "    async_json = loads(async_results)\n",
    "    print(dumps(async_json, indent=2))\n",
    "    async_jobId = async_json['jobID']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format and display the complete url to the Harmony API job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_url = f'https://{harmony}/jobs/{async_jobId}'\n",
    "job_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for the job status and links in case the request is still processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    loop_response = request.urlopen(job_url)\n",
    "    loop_results = loop_response.read()\n",
    "    job_json = loads(loop_results)\n",
    "    if job_json['status'] != 'running':\n",
    "        break\n",
    "    print(f\"# Job status is running. Progress is {job_json['progress']}. Trying again.\")\n",
    "    time.sleep(5)\n",
    "\n",
    "links = []\n",
    "if job_json['status'] == 'successful' and job_json['progress'] == 100:\n",
    "    print(\"# Job progress is 100%. Links to job outputs are displayed below:\")\n",
    "    links = [link['href'] for link in job_json['links']]\n",
    "    display(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access url for the output zarr file**\n",
    "\n",
    "The new zarr dataset is staged for us in an S3 bucket. The url is the last one in the list shown above.\n",
    "\n",
    "Select the url and display below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_url = links[-1]\n",
    "zarr_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Access credentials for the output zarr file**\n",
    "\n",
    "Credentials provided at the third and fourth urls in the list grant authenticated access to your staged S3 resources.\n",
    "\n",
    "Grab the credentials as a JSON string, load to a Python dictionary, and display their expiration date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with request.urlopen(f\"https://{harmony}/cloud-access\") as f:\n",
    "    creds = loads(f.read())\n",
    "\n",
    "creds['Expiration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr file with *s3fs*\n",
    "\n",
    "We use the AWS `s3fs` package to get metadata about the zarr data store and list its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_fs = s3fs.S3FileSystem(\n",
    "    key=creds['AccessKeyId'],\n",
    "    secret=creds['SecretAccessKey'],\n",
    "    token=creds['SessionToken'],\n",
    "    client_kwargs={'region_name':'us-west-2'},\n",
    ")\n",
    "zarr_store = zarr_fs.get_mapper(root=zarr_url, check=False)\n",
    "zarr_dataset = zarr.open(zarr_store)\n",
    "\n",
    "print(zarr_dataset.tree())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print metadata for the *lwe_thickness* variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zarr_dataset.lwe_thickness.info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open staged zarr file with *xarray*\n",
    "\n",
    "Here's the documentation for `xarray`'s zarr reader: http://xarray.pydata.org/en/stable/generated/xarray.open_zarr.html\n",
    "\n",
    "Open the zarr dataset and print the dataset \"header\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_GRACE = xr.open_zarr(zarr_store)\n",
    "print(ds_GRACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subset by Latitude/Longitude**\n",
    "\n",
    "Once we have obtained all the data, to make processing quicker, we are going to subset datasets by latitude/longitude for the Amazon River estuary.\n",
    "\n",
    "Once we have obtained the GRACE-FO data, we should spatial subset the data to the minimal area covering the Amazon River estuary. This will reduce processing load and reduce cloud costs for the user.\n",
    "\n",
    "Make a GRACE-FO subset and display the min, max of the *lat* and *lon* variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_GRACE = ds_GRACE.sel(lat=slice(-18, 10), lon=slice(275, 330))\n",
    "print(subset_GRACE.lat.min().data, \n",
    "      subset_GRACE.lat.max().data,\n",
    "      subset_GRACE.lon.min().data,\n",
    "      subset_GRACE.lon.max().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select the variable for Land Water Equivalent Thickness (*lwe_thickness*)**\n",
    "\n",
    "Grab the land water equivalent thickness variable from the GRACE subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwe = subset_GRACE.lwe_thickness\n",
    "print(lwe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "We will create an animation from sequential GRACE-FO plots over the Amazon Rainforest in the following cells. Define two functions to make the process a bit more convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:47:58.175973Z",
     "start_time": "2020-11-20T21:47:58.172032Z"
    }
   },
   "outputs": [],
   "source": [
    "def setup_map(ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    title = str(pd.to_datetime(ds_subset.time[t].values))\n",
    "    pmap.set_title(title, fontsize=14)\n",
    "    pmap.coastlines()\n",
    "    pmap.set_extent(extent)\n",
    "    pmap.add_feature(cartopy.feature.RIVERS)\n",
    "    variable_desired = var[t,:,:]\n",
    "    cont = pmap.contourf(x, y, variable_desired, cmap=cmap, levels=levels, zorder=1)\n",
    "    return cont\n",
    "\n",
    "def animate_ts(framenumber, ax, pmap, ds_subset, x, y, var, t, cmap, levels, extent):\n",
    "    cont = setup_map(ax, pmap, ds_subset, x, y, var, t + framenumber, cmap, levels, extent) \n",
    "    return cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the first timestep in the JPL GRACE/GRACE-FO Mascon time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:48:01.317686Z",
     "start_time": "2020-11-20T21:48:00.783305Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a matplotlib plot object and add subplot:\n",
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# Configure axes to display projected data using PlateCarree crs:\n",
    "pmap = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "# Get arrays of x and y to label the plot axes:\n",
    "x,y = np.meshgrid(subset_GRACE.lon, subset_GRACE.lat)                        \n",
    "\n",
    "# Set a few constants for plotting the GRACE-FO time series:\n",
    "time_start  = 168\n",
    "cmap_name   = \"bwr_r\"\n",
    "cmap_levels = np.linspace(-100., 100., 14)\n",
    "map_extent  = [-85, -30, -16, 11]\n",
    "\n",
    "# Plot the first timestep: \n",
    "cont = setup_map(ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent)\n",
    "\n",
    "fig.colorbar(cont, ticks=cmap_levels, orientation='horizontal', label='Land Water Equivalent Thickness (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the 2019 timesteps sequentially to create an animation of land water equivalent thickness for the Amazon Rainforest territories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:48:06.741616Z",
     "start_time": "2020-11-20T21:48:04.843045Z"
    }
   },
   "outputs": [],
   "source": [
    "ani = animation.FuncAnimation(fig, animate_ts, frames=range(0,12), fargs=(\n",
    "    ax, pmap, subset_GRACE, x, y, lwe, time_start, cmap_name, cmap_levels, map_extent\n",
    "), interval=500)\n",
    "\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User note: You will need to install 'ffmpeg' in the cmd prompt to save the .mpg to disk. Use the following command to install from the conda-forge channel:\n",
    "\n",
    "```shell\n",
    "conda install -c conda-forge ffmpeg\n",
    "```\n",
    "\n",
    "Uncomment, run the next cell to save the animation to MP4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:48:17.741846Z",
     "start_time": "2020-11-20T21:48:14.627311Z"
    }
   },
   "outputs": [],
   "source": [
    "#ani.save(\"tutorial7_animation_GRACEFO.mp4\", writer=animation.FFMpegWriter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-prem hydro data from Pre-SWOT MEaSUREs program\n",
    "\n",
    "Data from [**PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2**](https://podaac.jpl.nasa.gov/dataset/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2) are not currently available on the cloud, but we can access via the PO.DAAC's on-prem OPeNDAP service (Hyrax) instead.\n",
    "\n",
    "<img src=\"https://podaac.jpl.nasa.gov/Podaac/thumbnails/PRESWOT_HYDRO_GRRATS_L2_DAILY_VIRTUAL_STATION_HEIGHTS_V2.jpg\" width=\"55%\">\n",
    "\n",
    "The guidebook explains the details of the Pre-SWOT MEaSUREs data: https://podaac-tools.jpl.nasa.gov/drive/files/allData/preswot_hydrology/L2/rivers/docs/GRRATS_user_handbookV2.pdf\n",
    "\n",
    "**Access URL for PO.DAAC on-prem OPeNDAP service**\n",
    "\n",
    "Identify an appropriate OPeNDAP endpoint through the following steps:\n",
    "\n",
    "1. Go to the project/mission page on the PO.DAAC portal (e.g. for Pre-SWOT MEaSUREs: https://podaac.jpl.nasa.gov/MEaSUREs-Pre-SWOT)\n",
    "\n",
    "2. Choose the dataset of interest. Go to the \"Data Access\" tab of the corresponding dataset landing page, which should like the OPeNDAP access link (for compatible datasets, e.g. for the daily river heights from virtual stations: https://podaac-opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/).\n",
    "\n",
    "3. Navigate to the desired NetCDF file and copy the endpoint (e.g. for our Amazon Basin use case we choose the South America file: https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc).\n",
    "\n",
    "### Open netCDF file with *xarray*\n",
    "\n",
    "Open the netCDF dataset via OPeNDAP using *xarray*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:51:48.641119Z",
     "start_time": "2020-11-20T21:51:48.628878Z"
    }
   },
   "outputs": [],
   "source": [
    "ds_MEaSUREs = xr.open_dataset('https://opendap.jpl.nasa.gov/opendap/allData/preswot_hydrology/L2/rivers/daily/South_America_Amazon1kmdaily.nc')\n",
    "print(ds_MEaSUREs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our desired variable is height (meters above EGM2008 geoid) for this exercise, which can be subset by distance and time. Distance represents the distance from the river mouth, in this example, the Amazon estuary. Time is between April 8, 1993 and April 20, 2019.\n",
    "\n",
    "### Plot\n",
    "\n",
    "**Amazon River heights for March 16, 2018**\n",
    "\n",
    "Plot the river distances and associated heights on the map at time t=9069:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T21:51:55.078273Z",
     "start_time": "2020-11-20T21:51:54.929376Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=[13,9]) \n",
    "ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "ax.coastlines()\n",
    "ax.set_extent([-85, -30, -20, 20])\n",
    "ax.add_feature(cartopy.feature.RIVERS)\n",
    "\n",
    "plt.scatter(ds_MEaSUREs.lon, ds_MEaSUREs.lat, lw=1, c=ds_MEaSUREs.height[:,9069])\n",
    "plt.colorbar(label='Interpolated River Heights (m)')\n",
    "plt.clim(-10,100)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 1/2019 to 12/2019 at lat/lon (-0.7, -50). For the 2019 year, measurements of LWE thickness followd expected patterns of high volume of water from the river output into the estuary.\n",
    "\n",
    "**2011-2019 Seasonality Plots (WIP)**\n",
    "\n",
    "For GRACE-FO, plotting lwe_thickness[107:179,34,69] indicates time, latitude, and longitude indices corresponding to the pixel for the time period 8/2011 to 12/2019 at lat/lon (-0.7, -50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot variables associated with river\n",
    "fig, ax1 = plt.subplots(figsize=[12,7])\n",
    "#plot river height\n",
    "ds_MEaSUREs.height[16,6689:9469].plot(color='darkblue')\n",
    "\n",
    "#plot LWE thickness on secondary axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(subset_GRACE.time[107:179], subset_GRACE.lwe_thickness[107:179,34,69], color = 'darkorange')\n",
    "\n",
    "ax1.set_xlabel('Date')\n",
    "ax2.set_ylabel('Land Water Equivalent Thickness (cm)', color='darkorange')\n",
    "ax1.set_ylabel('River Height (m)', color='darkblue')\n",
    "ax2.legend(['GRACE-FO'], loc='upper right')\n",
    "ax1.legend(['Pre-SWOT MEaSUREs'], loc='lower right')\n",
    "\n",
    "plt.title('Amazon Estuary, 2011-2019 Lat, Lon = (-0.7, -50)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.997px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
